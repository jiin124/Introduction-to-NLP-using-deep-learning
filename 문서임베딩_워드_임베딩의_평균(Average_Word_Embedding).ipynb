{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "문서임베딩 : 워드 임베딩의 평균(Average Word Embedding).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPew/SNToTtaqX7L0ZGJXox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiin124/Introduction-to-NLP-using-deep-learning/blob/main/%EB%AC%B8%EC%84%9C%EC%9E%84%EB%B2%A0%EB%94%A9_%EC%9B%8C%EB%93%9C_%EC%9E%84%EB%B2%A0%EB%94%A9%EC%9D%98_%ED%8F%89%EA%B7%A0(Average_Word_Embedding).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "특정 문장 내 단어들의 임베딩 벡터들의 평균이 그 문장의 벡터가 될 수 있다. 이번에는 임베딩이 잘 된 상황에서는 단어 벡터들의 평균 만드로 텍스트 분류를 수행할 수 있음을 보이고, 워드 임베딩의 중요성을 이해해보겠다. \n",
        "\n",
        "영화 사이트 IMDB 영화 리뷰 데이터는 리뷰 텍스트에 리뷰가 긍정인 경우1, 부저인 경우 0으로 레이블링 한 데이터로 25000개의 훈련 데이터와 테스트 데이터 25000개로 구성된 데이터이다. "
      ],
      "metadata": {
        "id": "g5evByW8_OGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.데이터 로드와 전처리\n",
        "\n"
      ],
      "metadata": {
        "id": "HpJ9Jm1m_8OC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N0ckVE5r_EGK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 로드할 때 파라미터로 num_words를 사용하면 이 데이터에서 등장 빈도 순위로 몇 번째에 해당하는 단어까지를 사용할 것인지를 의미합니다. 만약 vocab_size를 20,000으로 지정할 경우 등장 빈도 순위가 20,000등이 넘는 단어들은 데이터를 로드할 때 전부 제거 후 로드합니다."
      ],
      "metadata": {
        "id": "PfpuoDlPAPAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=20000\n",
        "\n",
        "(X_train,y_train),(X_test,y_test)=imdb.load_data(num_words=vocab_size)\n",
        "print('훈련용 리뷰 개수:',len(X_train))\n",
        "print('테스트용 리뷰 개수:',len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7UmC-fFAHKH",
        "outputId": "7352351f-e5c9-495a-ca0c-f63496371a8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "훈련용 리뷰 개수: 25000\n",
            "테스트용 리뷰 개수: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 데이터는 이미 정수 인코딩까지의 전처리가 진행되어져 있다. 그래서 단어 집합을 만들고 각 단어를 정수로 인코딩하는 과정을 할 필요가 없다. "
      ],
      "metadata": {
        "id": "pWsYuBeqAgk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 첫번째 샘플 :',X_train[0])\n",
        "print('훈련 데이터의 첫번째 샘플의 레이블 :',y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCOou-m3AbM3",
        "outputId": "542c4663-e148-4ceb-8645-59b3f97ea9cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 첫번째 샘플 : [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "훈련 데이터의 첫번째 샘플의 레이블 : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정수 1이 출력되는데 이는 첫번째 리뷰가 긍정 리뷰임을 의미한다. 각 리뷰의 평균 길이를 계산해보자. "
      ],
      "metadata": {
        "id": "rB4XX0XQAtuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련용 리뷰의 평균 길이: {}'.format(np.mean(list(map(len,X_train)),dtype=int)))\n",
        "print('테스트용 리뷰의 평균 길이: {}'.format(np.mean(list(map(len,X_test)),dtype=int)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zJBBxIRHz8Y",
        "outputId": "b90d168c-8134-472b-8883-c5c9948f4f6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련용 리뷰의 평균 길이: 238\n",
            "테스트용 리뷰의 평균 길이: 230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=400\n",
        "\n",
        "X_train=pad_sequences(X_train,maxlen=max_len)\n",
        "X_test=pad_sequences(X_test,maxlen=max_len)\n",
        "print('X_train의 크기(shape) :',X_train.shape)\n",
        "print('X_test의 크기(shape) :',X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5vpFbyFAsRm",
        "outputId": "5d709946-f80e-4f1e-f0a7-a2909c56e80f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train의 크기(shape) : (25000, 400)\n",
            "X_test의 크기(shape) : (25000, 400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 모델 설계하기\n",
        "\n",
        "임베딩 벡터를 평균으로 사용하는 모델을 설계해 보자. GlobalAveragePooling1D()는 입력으로 들어오는 모든 벡터들의 평균을 구하는 역할을 한다. Ebbedding()다음에 BlobalAveragePooling1D()를 추가하면 해당 문장의 모든 단어 벡터들의 평균 벡터를 구한다. \n",
        "\n",
        "이진 분류를 수행해야하므로 그 후에는 시그모이드 함수를 활성화 함수로 사용하는 뉴런 1개를 배치한다. 훈련데이터의 20%를 검증 데이터로 사용하고 총 10 에포크 학습한다. "
      ],
      "metadata": {
        "id": "Malg0qtqJ0Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Dense,Embedding,GlobalAveragePooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "\n",
        "embedding_dim=64\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size,embedding_dim))\n",
        "\n",
        "#모든 단어 벡터의 평균을 구한다. \n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "es=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=4)\n",
        "mc=ModelCheckpoint('embedding_average_model.h5',monitor='val_acc',mode='max',verbose=1,save_best_only=True)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "model.fit(X_train,y_train,batch_size=32,epochs=10,callbacks=[es,mc],validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpT-wEg3Gi5U",
        "outputId": "e9a05466-2e88-45c7-e7da-cd7af74bc10f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.6277 - acc: 0.7256\n",
            "Epoch 00001: val_acc improved from -inf to 0.81780, saving model to embedding_average_model.h5\n",
            "625/625 [==============================] - 14s 21ms/step - loss: 0.6277 - acc: 0.7256 - val_loss: 0.5199 - val_acc: 0.8178\n",
            "Epoch 2/10\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.4295 - acc: 0.8572\n",
            "Epoch 00002: val_acc improved from 0.81780 to 0.86820, saving model to embedding_average_model.h5\n",
            "625/625 [==============================] - 11s 18ms/step - loss: 0.4293 - acc: 0.8572 - val_loss: 0.3786 - val_acc: 0.8682\n",
            "Epoch 3/10\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.8924\n",
            "Epoch 00003: val_acc improved from 0.86820 to 0.87980, saving model to embedding_average_model.h5\n",
            "625/625 [==============================] - 11s 18ms/step - loss: 0.3183 - acc: 0.8924 - val_loss: 0.3233 - val_acc: 0.8798\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2612 - acc: 0.9100\n",
            "Epoch 00004: val_acc improved from 0.87980 to 0.88060, saving model to embedding_average_model.h5\n",
            "625/625 [==============================] - 12s 18ms/step - loss: 0.2612 - acc: 0.9100 - val_loss: 0.3027 - val_acc: 0.8806\n",
            "Epoch 5/10\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9230\n",
            "Epoch 00005: val_acc improved from 0.88060 to 0.88700, saving model to embedding_average_model.h5\n",
            "625/625 [==============================] - 12s 18ms/step - loss: 0.2238 - acc: 0.9230 - val_loss: 0.2853 - val_acc: 0.8870\n",
            "Epoch 6/10\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9343\n",
            "Epoch 00006: val_acc improved from 0.88700 to 0.89260, saving model to embedding_average_model.h5\n",
            "625/625 [==============================] - 12s 18ms/step - loss: 0.1952 - acc: 0.9343 - val_loss: 0.2756 - val_acc: 0.8926\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.1719 - acc: 0.9440\n",
            "Epoch 00007: val_acc did not improve from 0.89260\n",
            "625/625 [==============================] - 10s 16ms/step - loss: 0.1719 - acc: 0.9440 - val_loss: 0.2741 - val_acc: 0.8918\n",
            "Epoch 8/10\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9508\n",
            "Epoch 00008: val_acc improved from 0.89260 to 0.89340, saving model to embedding_average_model.h5\n",
            "625/625 [==============================] - 10s 16ms/step - loss: 0.1523 - acc: 0.9509 - val_loss: 0.2713 - val_acc: 0.8934\n",
            "Epoch 9/10\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9572\n",
            "Epoch 00009: val_acc improved from 0.89340 to 0.89500, saving model to embedding_average_model.h5\n",
            "625/625 [==============================] - 10s 16ms/step - loss: 0.1345 - acc: 0.9571 - val_loss: 0.2743 - val_acc: 0.8950\n",
            "Epoch 10/10\n",
            "622/625 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9629\n",
            "Epoch 00010: val_acc improved from 0.89500 to 0.89680, saving model to embedding_average_model.h5\n",
            "625/625 [==============================] - 10s 16ms/step - loss: 0.1193 - acc: 0.9628 - val_loss: 0.2783 - val_acc: 0.8968\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f45ae923910>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model=load_model('embedding_average_model.h5')\n",
        "print('\\n 테스트 정확도: %.4f'%(loaded_model.evaluate(X_test,y_test)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vnf4sJFL_mj",
        "outputId": "57772146-66b8-40c0-aafd-3c68417ea306"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 3s 3ms/step - loss: 0.2931 - acc: 0.8864\n",
            "\n",
            " 테스트 정확도: 0.8864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9Ic4AD02dcZU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}