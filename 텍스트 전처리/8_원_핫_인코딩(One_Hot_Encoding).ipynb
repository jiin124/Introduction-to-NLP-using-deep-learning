{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_원-핫 인코딩(One-Hot Encoding)",
      "provenance": [],
      "authorship_tag": "ABX9TyN15+LbrRyCqoDtcsUhfrqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiin124/Introduction-to-NLP-using-deep-learning/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EC%A0%84%EC%B2%98%EB%A6%AC/8_%EC%9B%90_%ED%95%AB_%EC%9D%B8%EC%BD%94%EB%94%A9(One_Hot_Encoding).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 집합(vocabulary) 에 대해서 정의해보겠습니다. 단어 집합은 앞으로 자연어 처리에서 계속 나오는 개념이기 때문에 여기서 이해하고 가야합니다. 단어 집합은 서로 다른 단어들의 집합입니다. 여기서 혼동이 없도록 서로 다른 단어라는 정의에 대해서 좀 더 주목할 필요가 있습니다. 단어 집합(vocabulary)에서는 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주합니다. 이 책에서는 앞으로 단어 집합에 있는 단어들을 가지고, 문자를 숫자. 더 구체적으로는 벡터로 바꾸는 원-핫 인코딩을 포함한 여러 방법에 대해서 배우게 됩니다."
      ],
      "metadata": {
        "id": "8HuY98Obj5fI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 원-핫 인코딩 이란?\n",
        "\n",
        "원핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다. 이렇게 표현된 벡터를 원-핫 벡터라고 한다. \n",
        "\n",
        "<원-핫 인코딩 과정>\n",
        "1. 정수 인코딩을 수행, 각 단어에 고유한 정수 부여.\n",
        "2. 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고 다른 단어의 인덱스의 위치에는 0을 부여. \n",
        "\n",
        "예시로 원-핫 벡터를 만들어보겠다. \n",
        "\n",
        "문장 : 나는 자연어 처리를 배운다\n",
        "\n",
        "Okt 형태소 분석기를 통해 문장에 대해서 토큰화를 수행한다. \n"
      ],
      "metadata": {
        "id": "t5ZSyMr3lV_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g7bPUaumxWg",
        "outputId": "a6dca6e5-7690-456c-da83-8955629e88d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 52.2 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 62.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSie8UmLjqGD",
        "outputId": "11775ad2-d223-4132-aec5-98c611afd2d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "okt=Okt()\n",
        "tokens=okt.morphs(\"나는 자연어 처리를 배운다\")\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 토큰에 대해서 고유한 정수 부여"
      ],
      "metadata": {
        "id": "ad9Knla-m29x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index={word:index for index, word in enumerate(tokens)}\n",
        "print('단어 집합:',word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg0NTTFTmMPa",
        "outputId": "f42a353d-1b61-4ada-d3e0-71ef81f9219d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합: {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(word,word_to_index):\n",
        "  one_hot_vector=[0]*(len(word_to_index))\n",
        "  index=word_to_index[word]\n",
        "  one_hot_vector[index]=1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "FOA8FSTem-ok"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'자연어'라는 단어의 원-핫벡터를얻어보자"
      ],
      "metadata": {
        "id": "Q0ivPapin8q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding(\"자연어\",word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws2cqqpinhhH",
        "outputId": "be6ac015-8775-4eaa-e748-9cb0d3e86152"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 케라스를 이용한 원-핫 인코딩\n",
        "\n",
        "케라스에서는 원-핫 인코딩을 수행하는 유용한 도구 to_categorical()를 지원한다. 이번에는 케라스만으로 정수 인코딩과 원핫인코딩을 순차적으로 진행해보겠다. "
      ],
      "metadata": {
        "id": "hG2PYct1oJEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
      ],
      "metadata": {
        "id": "yQSjWAhQoEMS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g0ZYxdWoU3x",
        "outputId": "094a9f62-78fc-4063-f80a-e8d29e5db500"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트인 sub_text를 만들어 확인해보겠습니다."
      ],
      "metadata": {
        "id": "pGgotmKDpJ6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlRfsv-so4QN",
        "outputId": "d44fd760-949f-4deb-9747-8f5df014ac14"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 5, 1, 6, 3, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스는 정수 인코딩 된 결과로부터 원-핫 인코딩을 수행하는` to_categorical()`를 지원합니다."
      ],
      "metadata": {
        "id": "oxvb4VNEpX2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot=to_categorical(encoded)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMOuMb5XpObn",
        "outputId": "60a3693b-6c93-412e-e719-4c161a5436d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 결과는 \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"라는 문장이 [2, 5, 1, 6, 3, 7]로 정수 인코딩이 되고나서, 각각의 인코딩 된 결과를 인덱스로 원-핫 인코딩이 수행된 모습을 보여줍니다."
      ],
      "metadata": {
        "id": "wIGIdQIFppNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 원핫인코딩의 한계\n",
        "\n",
        "단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있다. 벡터의 차원이 늘어난다는 것이다. \n",
        "\n",
        "또한 원-핫 벡터는 단어의 유사도를 표현하지 못한다는 단점이 있다.  예를 들어서 늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해서 원-핫 인코딩을 해서 각각, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]이라는 원-핫 벡터를 부여받았다고 합시다. 이때 원-핫 벡터로는 강아지와 늑대가 유사하고, 호랑이와 고양이가 유사하다는 것을 표현할 수가 없습니다. 좀 더 극단적으로는 강아지, 개, 냉장고라는 단어가 있을 때 강아지라는 단어가 개와 냉장고라는 단어 중 어떤 단어와 더 유사한지도 알 수 없습니다.\n",
        "\n",
        "단어 간 유사성을 알 수 없다는 단점은 검색 시스템 등에서는 문제가 될 소지가 있습니다. 가령, 여행을 가려고 웹 검색창에 '삿포로 숙소'라는 단어를 검색한다고 합시다. 제대로 된 검색 시스템이라면, '삿포로 숙소'라는 검색어에 대해서 '삿포로 게스트 하우스', '삿포로 료칸', '삿포로 호텔'과 같은 유사 단어에 대한 결과도 함께 보여줄 수 있어야 합니다. 하지만 단어간 유사성을 계산할 수 없다면, '게스트 하우스'와 '료칸'과 '호텔'이라는 연관 검색어를 보여줄 수 없습니다.\n",
        "\n",
        "이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지가 있습니다. 첫째는 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있으며, 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있습니다. 그리고 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 GloVe라는 방법이 존재합니다."
      ],
      "metadata": {
        "id": "rnE1_WeXps2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-LtypVHbpity"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}