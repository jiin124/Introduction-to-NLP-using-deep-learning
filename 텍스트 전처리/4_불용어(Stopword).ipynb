{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_불용어(Stopword)",
      "provenance": [],
      "authorship_tag": "ABX9TyOWmZGwcTHfyPxOzRsPeXH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiin124/Introduction-to-NLP-using-deep-learning/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EC%A0%84%EC%B2%98%EB%A6%AC/4_%EB%B6%88%EC%9A%A9%EC%96%B4(Stopword).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해 큰 의미가 없는 단어 토큰을 제거하는 작업. 여기서 큰 의미가 없다는 것으 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말한다. \n",
        "예를들어 I,my,me,over 조사 접미사 같은 단어들은 문장에서 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 있다. 이런 단어를 불용어(stopword)라고 하며 NLTK에서는 위와 같은 100여개의 이상의 영어단어들을 불용어로 패키지 내에서 미리정의하고 있다. \n",
        "\n"
      ],
      "metadata": {
        "id": "gNTaRVsQBq9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuKQBlegCob6",
        "outputId": "fcc08b12-4680-4d4b-b650-fd144b4de756"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 7.3 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHplq4IeDNyY",
        "outputId": "fd7e4e5e-b00c-47b9-e79d-4a14192109e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhuoyIzAEajQ",
        "outputId": "f43d758a-d124-4946-e753-97d13aa30938"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E5KSmpiWBne1"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from konlpy.tag import Okt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. NLTK에서 불용어 확인하기"
      ],
      "metadata": {
        "id": "eYWDkesJCugz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_list=stopwords.words('english')\n",
        "print('불용어 개수 :',len(stop_words_list))\n",
        "print('불용어 10개 출력 :',stop_words_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG2SLGL1Citf",
        "outputId": "7499dfb8-2057-437d-83f5-c3b9db51462f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. NLTK를 통해서 불용어 제거하기\n"
      ],
      "metadata": {
        "id": "uwNTptYjDSEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words=set(stopwords.words('english'))#불용어 집합 \n",
        "\n",
        "word_tokens=word_tokenize(example)#example 문장 토큰화\n",
        "\n",
        "results=[]\n",
        "for word in word_tokens:\n",
        "  if word not in stop_words:\n",
        "    results.append(word)\n",
        "\n",
        "print('불용어 제거 전:',word_tokens)\n",
        "print('불용어 제거 후:',results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t5JYMHrDK5t",
        "outputId": "637206c1-27eb-45a7-9604-d408c979f6ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전: ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후: ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 한국어에서 불용어 제거하기\n",
        "\n",
        "한국어에서 불용어를 제거하는 방법으로는 간단하게는 토큰화 후에 조사, 접속사 등을 제거하는 방법이 있다. 하지만 불용어를 제거하려고 하다보면 조사나 접속사와 같은 단어들뿐만아니라 명사, 형용사 같은 단어들 중에서 불용어로서 제거하고 싶은 단어들이 생기기도 한다. 결국 사용자가 직접 불용어 사전을 만들게 되는 경우가 있다. \n",
        "\n",
        "직접 불용어를 정의해보고 주어진 문장으로부터 사용자가 정의한 불용어 사전으로부터 불용어를 제거해보겠다. "
      ],
      "metadata": {
        "id": "1sDsBhiEFQIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt=Okt()\n",
        "\n",
        "example=\"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words=\"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words=set(stop_words.split(' '))#불용어 사전 \n",
        "word_tokens=okt.morphs(example)#예시 문장 형태소 나누기\n",
        "\n",
        "result=[word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print('불용어 제거 전:',word_tokens)\n",
        "print('불용어 제거 후:',result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhsCLgEAEYcS",
        "outputId": "025330f8-6b1d-4c62-c62b-b83f11fff10c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전: ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후: ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.ranks.nl/stopwords/korean\n",
        "\n",
        "보편적으로 선택할 수 있는 한국어 불용어 리스트 . 절대적인 기준은 아님"
      ],
      "metadata": {
        "id": "Mv7Yy_gsIiUL"
      }
    }
  ]
}