{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Bag of Words(BoW)",
      "provenance": [],
      "authorship_tag": "ABX9TyPz21DSV7kMpJ5F8tvV5EU4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiin124/Introduction-to-NLP-using-deep-learning/blob/main/4.%EC%B9%B4%EC%9A%B4%ED%8A%B8%20%EA%B8%B0%EB%B0%98%EC%9D%98%20%EB%8B%A8%EC%96%B4%20%ED%91%9C%ED%98%84/2_Bag_of_Words(BoW).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어의 등장 순서를 고려하지 않는 빈도수 기반의 단어 표현 방법인 Bag of Words에 대해서 학습. "
      ],
      "metadata": {
        "id": "jAek2iA2ZdmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Bag of Words란?\n",
        "\n",
        "단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법입니다. \n",
        "\n",
        "BoW 만들기\n",
        "- (1) 각 단어에 고유한 정수 인덱스를 부여합니다.  # 단어 집합 생성.\n",
        "- (2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.  \n",
        "\n",
        "예제를 통해 BoW 이해하기"
      ],
      "metadata": {
        "id": "awGZ60GkZiJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LyMB14jZ5Q8",
        "outputId": "307f47a1-7cad-4b2f-fcfc-b6f13d177313"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7D4E679KZZbp"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "okt=Okt()\n",
        "\n",
        "def build_bag_of_words(document):\n",
        "  #온점 제거 및 형태소 분석\n",
        "  document=document.replace('.','')\n",
        "  tokenized_document=okt.morphs(document)\n",
        "\n",
        "  word_to_index={}\n",
        "  bow=[]\n",
        "\n",
        "  for word in tokenized_document:\n",
        "    if word not in word_to_index.keys():\n",
        "      word_to_index[word]=len(word_to_index)\n",
        "      #BoW에 전부 기본값 1 을 넣는다.\n",
        "      bow.insert(len(word_to_index)-1,1)\n",
        "\n",
        "    else:\n",
        "      #재등장하는 단어의 인덱스\n",
        "      index=word_to_index.get(word)\n",
        "      #재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다.\n",
        "      bow[index]=bow[index]+1\n",
        "\n",
        "  return word_to_index,bow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
        "\n",
        "vocab,bow=build_bag_of_words(doc1)\n",
        "print('vocabulary:',vocab)\n",
        "print('bag of words vector :',bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mALnJUYocS08",
        "outputId": "e4997aaf-effc-4802-86e4-a8e39372a7ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary: {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
            "bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문서1에 각 단어에 대해서 인덱스를 부여한 결과는 첫번째 출력 결과입니다. 문서1의 BoW는 두번째 출력 결과입니다. 두번째 출력 결과를 보면, 인덱스 4에 해당하는 물가상승률은 두 번 언급되었기 때문에 인덱스 4에 해당하는 값이 2임입니다. 인덱스는 0부터 시작됨에 주의합니다. 다시 말해 물가상승률은 BoW에서 다섯번째 값입니다. 만약, 한국어에서 불용어에 해당되는 조사들 또한 제거한다면 더 정제된 BoW를 만들 수도 있습니다."
      ],
      "metadata": {
        "id": "PEnhTuvnd5Fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Bag of Words의 다른 예제들\n",
        "\n",
        "문서2 : 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다."
      ],
      "metadata": {
        "id": "FiHWjufGd8pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2='소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
        "\n",
        "vocab,bow=build_bag_of_words(doc2)\n",
        "print('vocabulary :',vocab)\n",
        "print('bag of words vector :',bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeP2eVsocct5",
        "outputId": "6b1a15c0-28fd-4b68-dab0-9749b3655c50"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
            "bag of words vector : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문서1과 문서2를 합쳐서 문서 3이라고 명명하고, BoW를 만들 수도 있습니다."
      ],
      "metadata": {
        "id": "CBa5L-taedBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문서3: 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다."
      ],
      "metadata": {
        "id": "vA6L0tyyea_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc3=doc1+' '+doc2\n",
        "\n",
        "vocag,bow=build_bag_of_words(doc3)\n",
        "print('vocabulary :',vocab)\n",
        "print('bag of words vector :',bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt7ZlMY8eLi5",
        "outputId": "3a5d0a2b-8ad4-4847-cfe6-702a80cb63e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
            "bag of words vector : [1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. CountVectorizer 클래스로 BoW만들기\n",
        "\n",
        "사이킷런에서는 단어의 빈도를 count해 Vector로 만드는 CountVecotrizer 클래스를 지원한다. 이를 이용하면 영어에 대해서는 BoW를 만들 수 있다. CountVectorizer로 간단하고 빠르게 BoW를 만드는 실습을 진행해보도록 하겠다. "
      ],
      "metadata": {
        "id": "XUUZzzj6e8tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "4_EMNzyafSQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = ['you know I want your love. because I love you.']\n",
        "vector=CountVectorizer()\n",
        "\n",
        "#코퍼스로 부터 각 단어의 빈도수를 기록\n",
        "print('bag of words vector :',vector.fit_transform(corpus).toarray())\n",
        "\n",
        "#각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
        "print('vocabulary :',vector.vocabulary_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy32AZFbeooA",
        "outputId": "009d1d2c-e637-42a0-a469-4d7a867445ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 2 1 2 1]]\n",
            "vocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예제 문장에서 you와 love는 두 번씩 언급되었으므로 각각 인덱스 2와 인덱스 4에서 2의 값을 가지며, 그 외의 값에서는 1의 값을 가지는 것을 볼 수 있습니다. 또한 알파벳 I는 BoW를 만드는 과정에서 사라졌는데, 이는 CountVectorizer가 기본적으로 길이가 2이상인 문자에 대해서만 토큰으로 인식하기 때문입니다. 정제(Cleaning) 챕터에서 언급했듯이, 영어에서는 길이가 짧은 문자를 제거하는 것 또한 전처리 작업으로 고려되기도 합니다."
      ],
      "metadata": {
        "id": "2EXkslYlg0mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 불용어를 제거한 BoW 만들기\n",
        "\n",
        "앞서 불용어는 자연어 처리에서 별로 의미를 갖지 않는 단어들이라고 언급한 바가 있다.\n",
        "\n",
        " BoW를 사용한다는 것은 그 문서에서 각 단어가 얼마나 자주 등장했는지를 보겠다는 것입니다. 그리고 각 단어에 대한 빈도수를 수치화 하겠다는 것은 결국 텍스트 내에서 어떤 단어들이 중요한지를 보고싶다는 의미를 함축하고 있습니다. 그렇다면 BoW를 만들때 불용어를 제거하는 일은 자연어 처리의 정확도를 높이기 위해서 선택할 수 있는 전처리 기법입니다.\n",
        "\n",
        " 영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원하고 있습니다."
      ],
      "metadata": {
        "id": "lMYPn5T4hAhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) 사용자가 직접 정의한 불용어 사용"
      ],
      "metadata": {
        "id": "tLyzoouLh4Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "_G9glDkAgLTX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"Family is not an important thing. It's everything.\"]\n",
        "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
        "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
        "print('vocabulary :',vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji5g21wdhvsT",
        "outputId": "762fe1bd-de25-4cd2-c9d6-99a8e0e5a7c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 1 1 1]]\n",
            "vocabulary : {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) CountVectorizer에서 제공하는 자체 불용어 사용\n",
        "\n"
      ],
      "metadata": {
        "id": "n76LWOqph8cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"Family is not an important thing. It's everything.\"]\n",
        "vect = CountVectorizer(stop_words=\"english\")\n",
        "print('bag of words vector :',vect.fit_transform(text).toarray())\n",
        "print('vocabulary :',vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A7oLWWdh1PH",
        "outputId": "f4610f0f-8c54-48d6-b3ef-3ca5b5ff75b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 1]]\n",
            "vocabulary : {'family': 0, 'important': 1, 'thing': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) NLTK에서 지원하는 불용어 사용\n"
      ],
      "metadata": {
        "id": "KKpxUnCQiC9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qiQef3_immp",
        "outputId": "fca514c5-5f4e-4b66-f77f-7aac1419fab2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"Family is not an important thing. It's everything.\"]\n",
        "stop_words = stopwords.words(\"english\")\n",
        "vect = CountVectorizer(stop_words=stop_words)\n",
        "print('bag of words vector :',vect.fit_transform(text).toarray()) \n",
        "print('vocabulary :',vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RVI_Tm7h_zs",
        "outputId": "432593a8-5433-417f-b44a-8ab337574090"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bag of words vector : [[1 1 1 1]]\n",
            "vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EcuN00RFikEV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}